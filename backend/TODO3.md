# TODO3.md - Phase 3.0: API Development & Beyond

This document tracks Phase 3.0 (API Development) and future phases of the SigmaSight Backend project.

**Moved from TODO2.md on 2025-08-26 to keep TODO2.md manageable**

---

## Phase 3.0: API Development
*All REST API endpoints for exposing backend functionality*

### 3.0.1 Batch Processing Admin APIs (from Section 1.6.8)
*Manual trigger endpoints for batch job execution and monitoring*

- [ ] **POST /api/v1/admin/batch/run-all** - Execute complete daily sequence
- [ ] **POST /api/v1/admin/batch/market-data** - Update market data only
- [ ] **POST /api/v1/admin/batch/aggregations** - Run portfolio aggregations
- [ ] **POST /api/v1/admin/batch/greeks** - Calculate Greeks only
- [ ] **POST /api/v1/admin/batch/factors** - Run factor analysis
- [ ] **POST /api/v1/admin/batch/risk-scenarios** - Execute risk scenarios
- [ ] **POST /api/v1/admin/batch/stress-tests** - Run stress testing
- [ ] **POST /api/v1/admin/batch/snapshots** - Generate snapshots
- [ ] **POST /api/v1/admin/batch/correlations** - Calculate correlations
- [ ] **GET /api/v1/admin/batch/status** - View job execution status
- [ ] **GET /api/v1/admin/batch/history** - View recent job execution history
- [ ] **GET /api/v1/admin/batch/schedule** - View upcoming scheduled jobs

### 3.0.2 Portfolio Management APIs (from Section 1.7)
- [ ] **GET /api/v1/portfolio** - Portfolio summary with exposures
- [ ] **GET /api/v1/portfolio/exposures** - Time-series exposure data
- [ ] **GET /api/v1/portfolio/performance** - P&L and performance metrics
- [ ] **POST /api/v1/portfolio/upload** - CSV upload endpoint
- [ ] Implement CSV parsing based on SAMPLE_CSV_FORMAT.md
- [ ] Add position type detection logic
- [ ] Implement exposure calculations (notional & delta-adjusted) - COMPLETED in Section 1.4.3

### 3.0.3 Position Management APIs (from Section 1.8)
- [ ] **GET /api/v1/positions** - List positions with filtering
- [ ] **GET /api/v1/positions/grouped** - Grouped positions (by type/strategy)
- [ ] **GET /api/v1/positions/{id}** - Individual position details
- [ ] **PUT /api/v1/positions/{id}/tags** - Update position tags
- [ ] **GET /api/v1/tags** - Tag management
- [ ] **POST /api/v1/tags** - Create new tags
- [ ] **GET /api/v1/strategies** - Strategy groupings
- [ ] Implement position grouping logic

### 3.0.4 Risk Analytics APIs (from Section 1.9)
- [ ] **GET /api/v1/risk/greeks** - Portfolio Greeks summary
- [ ] **POST /api/v1/risk/greeks/calculate** - Calculate Greeks on-demand
- [ ] **GET /api/v1/risk/factors** - Portfolio factor exposures (7-factor model)
- [ ] **GET /api/v1/risk/factors/positions** - Position-level factor exposures
- [ ] **GET /api/v1/risk/metrics** - Risk metrics (POSTPONED TO V1.5)
- [ ] Create Greeks aggregation logic (completed in Section 1.4.3)
- [ ] Implement delta-adjusted exposure calculations (completed in Section 1.4.3)
- [ ] Integrate Greeks with factor calculations (delta-adjusted exposures)

### 3.0.5 Correlation & Stress Testing APIs (from Section 1.9.5)
*API endpoints for Section 1.4.7 stress testing and Section 1.4.8 correlation features*

#### Factor Correlation APIs
- [ ] **GET /api/v1/risk/correlations/factors/matrix** - Factor correlation matrix with metadata
- [ ] **POST /api/v1/risk/correlations/factors/calculate** - Calculate factor correlations on-demand

#### Stress Testing APIs
- [ ] **GET /api/v1/risk/stress-testing/scenarios** - List available stress test scenarios
- [ ] **POST /api/v1/risk/stress-testing/direct-impact** - Calculate direct stress impact
- [ ] **POST /api/v1/risk/stress-testing/correlated-impact** - Calculate correlated stress impact
- [ ] **POST /api/v1/risk/stress-testing/comprehensive** - Run comprehensive stress test
- [ ] **GET /api/v1/risk/stress-testing/results/{portfolio_id}** - Get latest stress test results

#### Position Correlation APIs
- [ ] **GET /api/v1/risk/correlations/positions/metrics** - Portfolio-level correlation metrics
- [ ] **GET /api/v1/risk/correlations/positions/matrix** - Full pairwise correlation matrix  
- [ ] **POST /api/v1/risk/correlations/positions/calculate** - Trigger position correlation calculation

### 3.0.6 Factor Analysis APIs (from Section 1.10)
- [ ] **GET /api/v1/factors/definitions** - List factor definitions (completed in Section 1.2)
- [ ] **GET /api/v1/factors/exposures/{portfolio_id}** - Portfolio factor exposures
- [ ] **GET /api/v1/factors/exposures/{portfolio_id}/positions** - Position-level factor exposures
- [ ] **POST /api/v1/factors/calculate** - Calculate factor exposures (252-day regression)
- [ ] Implement 252-day regression factor calculations (7-factor model)
- [ ] Create factor regression analysis using statsmodels OLS
- [ ] Add factor performance attribution
- [ ] Store both position-level and portfolio-level factor exposures

### 3.0.7 Tag Management APIs (from Section 1.11)
- [ ] **GET /api/v1/tags** - List all tags
- [ ] **POST /api/v1/tags** - Create new tag
- [ ] **PUT /api/v1/positions/{id}/tags** - Update position tags
- [ ] **DELETE /api/v1/tags/{id}** - Delete tag
- [ ] Implement tag validation and limits

### 3.0.8 API Infrastructure (from Section 1.12)
- [ ] Add user activity logging
- [ ] Create data validation middleware
- [x] Add rate limiting (100 requests/minute per user) - COMPLETED
- [x] Polygon.io API rate limiting with token bucket algorithm - COMPLETED
- [ ] Set up request/response logging

**Phase 2 Implementation Notes:**
- All calculation engines are ALREADY COMPLETE (Phase 1)
- APIs simply expose existing functionality
- Focus on clean REST design and proper error handling
- Implement pagination for list endpoints
- Add filtering and sorting capabilities

---

## Phase 4: Advanced Features & Frontend Integration (Future)

### 4.0 Developer Experience & Onboarding
*Make the project easy to set up and contribute to*

#### 4.0.1 Developer Onboarding Improvements ⏳ **PLANNED**
*Streamline project setup to reduce friction for new contributors*

**Timeline**: 2-3 Days | **Priority**: High (Developer Productivity)

##### Day 1: Docker & Environment Setup Enhancement
- [ ] **Keep existing `docker-compose.yml`** with PostgreSQL only (Redis not needed - unused in application)
- [ ] **Enhance `.env.example`** with all required environment variables documented:
  - [ ] `DATABASE_URL=postgresql://sigmasight:sigmasight_dev@localhost/sigmasight_db`
  - [ ] `POLYGON_API_KEY=your_key_here` (with setup instructions)
  - [ ] Remove unused Redis configuration variables
  - [ ] All other config variables with explanations
- [ ] **Create `scripts/setup_dev_environment.py`** - automated setup script that:
  - [ ] Validates Python 3.11+ and uv installation
  - [ ] Checks Docker is running: `docker compose up -d`
  - [ ] Creates `.env` from `.env.example` if missing
  - [ ] Waits for PostgreSQL health check to pass
  - [ ] Runs database migrations: `uv run alembic upgrade head`
  - [ ] Seeds demo data: `python scripts/reset_and_seed.py seed`
  - [ ] Validates setup with comprehensive health checks

##### Day 2: Documentation & Quick Start Enhancement
- [ ] **Streamline README.md** by consolidating existing guides (WINDOWS_SETUP_GUIDE.md, WINDSURF_SETUP.md):
  - [ ] Add unified "5-Minute Quick Start" section at the top
  - [ ] Simplify to: `git clone → docker compose up -d → ./scripts/setup_dev_environment.py`
  - [ ] Include demo user credentials prominently: 
    - `demo_individual@sigmasight.com / demo12345`
    - `demo_hnw@sigmasight.com / demo12345` 
    - `demo_hedgefundstyle@sigmasight.com / demo12345`
  - [ ] Move platform-specific details to appendix
- [ ] **Enhance existing setup guides** rather than creating new ones:
  - [ ] Update WINDOWS_SETUP_GUIDE.md to use automated setup script
  - [ ] Update WINDSURF_SETUP.md to reference new quick start
  - [ ] Ensure all guides point to the same streamlined workflow
- [ ] **Add `scripts/validate_environment.py`** - health check script for troubleshooting

##### Day 3: Developer Tools & Polish
- [ ] **Create `Makefile`** with common development commands:
  ```makefile
  setup: # Complete development environment setup
  seed: # Seed database with demo data
  test: # Run test suite
  lint: # Run linting and type checking
  clean: # Clean up containers and temp files
  ```
- [ ] **Add development health check endpoint** - `/health` with database status only
- [ ] **Create troubleshooting guide** for common setup issues:
  - [ ] Database connection problems
  - [ ] Migration failures
  - [ ] Missing environment variables
  - [ ] Docker connectivity issues

**Success Criteria**:
- ✅ New developer can go from `git clone` to working demo in under 5 minutes
- ✅ Single command setup: `make setup` or `./scripts/setup_dev_environment.py`
- ✅ Clear error messages with actionable solutions
- ✅ Comprehensive documentation for all setup scenarios
- ✅ Demo data works immediately after setup

**Cross-Reference**:
- Builds on production-ready seeding from Section 1.5.1
- Leverages existing `scripts/reset_and_seed.py` validation
- Supports all 8 batch calculation engines and 3 demo portfolios

---

### 4.1 Code Quality & Technical Debt
*Refactoring, deprecations, and technical improvements*

#### 4.1.1 Greeks Calculation Simplification
- [x] **Remove py_vollib dependency and fallback logic** - **COMPLETED**
  - [x] Remove `py-vollib>=1.0.1` from `pyproject.toml`
  - [x] Remove py_vollib imports and fallback code in `app/calculations/greeks.py`
  - [x] Remove `get_mock_greeks()` function - no more mock calculations
  - [x] Simplify Greeks calculation to use **mibian only**
  - [x] Return `None`/`NULL` values with logged errors if mibian fails
  - [x] Update unit tests to remove mock Greeks test cases
  - [x] Update function name: `calculate_greeks_hybrid()` → `calculate_position_greeks()`
  - [x] Update imports in `__init__.py` and `batch_orchestrator_v2.py`
  - [x] Run `uv sync` to clean py_vollib from environment
  - [x] Test end-to-end with demo data (mibian delta: 0.515 for ATM call)
  - **Rationale**: Eliminate warning messages and simplify codebase by relying solely on the proven mibian library
  - **Result**: ✅ **Successfully eliminated** py_vollib warnings, reduced complexity, maintained calculation quality
  - **Behavioral Changes**:
    - Stock positions now return `None` (Greeks not applicable)
    - Failed calculations return `None` with error logging
    - Options calculations use mibian-only (same quality, no fallbacks)

#### 4.1.2 Stress Test Model Architectural Improvement 🔴 **CRITICAL**
*Redesign stress test calculation to fix fundamental exposure multiplication issue*

**Timeline**: 3-5 Days | **Priority**: CRITICAL | **Created**: 2025-08-09

**Problem Context**:
The current stress test implementation has a fundamental flaw where each factor's exposure is calculated as `beta × full_portfolio_value`. This causes:
- Total factor exposures to exceed portfolio value (e.g., $5.4M exposure on $485K portfolio)
- Multi-factor scenarios to compound catastrophically (400%+ losses on unlevered portfolios)
- Mathematically impossible results that undermine system credibility

**Root Cause**:
```python
# Current flawed calculation in factors.py line 675:
exposure_dollar = float(beta_value) * float(portfolio_value)
# Each factor gets full portfolio × its beta, so 7 factors = 7× exposure!
```

**Improvement Options**:

1. **Quick Pragmatic Fix (Temporary)** ✅ **IMPLEMENTED 2025-08-09**
   - Cap losses at 99% of portfolio value
   - Scale factor impacts proportionally
   - Pros: Quick, prevents absurd losses
   - Cons: Not mathematically rigorous

2. **Normalize Factor Exposures** (Recommended Long-term)
   ```python
   total_beta = sum(abs(beta) for beta in all_factor_betas)
   normalized_exposure = (abs(beta) / total_beta) * portfolio_value
   ```
   - Pros: Exposures sum to portfolio value, mathematically sound
   - Cons: Changes exposure meaning, requires data migration

3. **Position-Level Stress Testing** (Most Accurate)
   ```python
   for position in positions:
       for factor, shock in shocked_factors.items():
           factor_exposure = get_position_factor_exposure(position, factor)
           position_loss += position.market_value * factor_exposure * shock
   ```
   - Pros: Most accurate, uses existing PositionFactorExposure data
   - Cons: More complex implementation, higher computation cost

4. **Weighted Factor Model**
   - Primary factor gets full exposure, secondary factors get partial weights
   - Pros: Reduces compounding while maintaining effects
   - Cons: Arbitrary weights, less theoretical grounding

**Implementation Tasks**:
- [ ] Analyze position-level factor exposures feasibility
- [ ] Design normalized exposure calculation
- [ ] Implement chosen solution (likely Option 2 or 3)
- [ ] Migrate historical stress test results
- [ ] Validate against known scenarios
- [ ] Update documentation and tests

**Success Criteria**:
- Maximum loss for unlevered portfolio ≤ 99% in worst case
- Factor exposures sum to ≤ portfolio value
- Results align with industry standard stress tests
- Historical scenarios produce believable losses

#### 4.0.2 Production Job Scheduling Architecture Decision ⏳ **RESEARCH NEEDED**
*Evaluate and select production-ready job scheduling solution*

**Timeline**: 1-2 Days | **Priority**: High (Production Readiness)

**Current State**: Using MemoryJobStore as temporary workaround for APScheduler greenlet errors

**Research Tasks**:
- [ ] **APScheduler Analysis**: Evaluate current limitations and APScheduler 4.x timeline
  - [ ] Document specific async/sync issues with current SQLAlchemy jobstore
  - [ ] Research APScheduler 4.x native async support availability and stability
  - [ ] Analyze job persistence requirements vs. current MemoryJobStore limitations
- [ ] **External Job Queue Options**: Research async-native alternatives
  - [ ] **Arq** - Redis-based async job queue, lightweight, FastAPI compatible
  - [ ] **Dramatiq** - Multi-broker async task queue with Redis/RabbitMQ support  
  - [ ] **Celery + async workers** - Traditional choice with recent async improvements
  - [ ] **RQ** - Simple Redis-based queue (sync, but could work with adaptation)
- [ ] **Infrastructure-Based Solutions**: Evaluate platform-native scheduling
  - [ ] **Kubernetes CronJobs** - Cloud-native scheduling with built-in monitoring
  - [ ] **Traditional cron + API endpoints** - Simple, reliable, OS-level scheduling
  - [ ] **Cloud provider solutions** - AWS EventBridge, GCP Cloud Scheduler, etc.
- [ ] **Hybrid Approaches**: Combine multiple strategies
  - [ ] External scheduler + internal job queue for complex workflows
  - [ ] API-triggered batch processing with external monitoring
  - [ ] Multi-tier approach: cron for scheduling + queue for execution

**Decision Criteria**:
- [ ] **Async Compatibility**: Native async support without greenlet errors
- [ ] **Job Persistence**: Survive application restarts and crashes  
- [ ] **Scalability**: Support multiple app instances and load balancing
- [ ] **Monitoring**: Job history, failure tracking, alerting capabilities
- [ ] **Operational Complexity**: Deployment, maintenance, debugging overhead
- [ ] **Development Timeline**: Implementation effort vs. production readiness needs

**Deliverables**:
- [ ] **Technical Comparison Matrix** - Feature comparison across all options
- [ ] **Architecture Recommendation** - Preferred solution with rationale  
- [ ] **Implementation Plan** - Migration steps from current MemoryJobStore
- [ ] **Rollback Strategy** - Fallback options if chosen solution has issues

**Notes**: Current MemoryJobStore works for development but lacks production reliability. Decision should balance immediate needs vs. long-term architecture goals.

#### 4.0.3 UUID Serialization Root Cause Investigation
- [ ] **Investigate asyncpg UUID serialization issue** 
  - **Background**: Multiple batch jobs fail with `'asyncpg.pgproto.pgproto.UUID' object has no attribute 'replace'`
  - **Current Status**: Working with pragmatic workaround (detects error and treats job as successful)
  - **Affected Areas Using Workaround**:
    - **Factor Analysis** (`_calculate_factors`) - UUID conversion in fresh DB session
    - **Market Risk Scenarios** (`_calculate_market_risk`) - UUID conversion for market beta/scenarios  
    - **Stress Testing** (`_run_stress_tests`) - UUID conversion for stress test execution
    - **Portfolio Snapshot** (`_create_snapshot`) - UUID conversion for snapshot creation
    - **Note**: All jobs work correctly when UUID type handling is applied
  - **Investigation Areas**:
    - Deep dive into asyncpg/SQLAlchemy UUID handling in batch context
    - Compare execution paths between direct function calls vs batch orchestrator
    - Identify where `.replace()` is being called on UUID objects
    - Determine if this is a library version compatibility issue
    - Analyze why portfolio_id parameter alternates between string and UUID types
  - **Workaround Pattern Applied**:
    ```python
    # UUID type safety pattern used in all affected jobs
    if isinstance(portfolio_id, str):
        portfolio_uuid = UUID(portfolio_id)
    else:
        portfolio_uuid = portfolio_id  # Already UUID object
    ```
  - **Success Criteria**: Either fix root cause or confirm workaround is the best long-term solution
  - **Priority**: Low (system is fully functional with workaround, all 8/8 jobs working)
  - **Reference**: Section 1.6.11 for comprehensive debugging history

#### 4.0.4 Technical Debt & Cleanup (Future)
- [ ] Standardize error handling patterns across all services
- [ ] Remove deprecated code comments and TODOs
- [ ] Consolidate similar utility functions
- [ ] Update Pydantic v1 validators to v2 field_validator syntax
- [ ] Review and optimize database query patterns
- [ ] Standardize logging levels and messages

#### 3.0.4 Performance Improvements (Future)
- [ ] Remove redundant database queries in position calculations
- [ ] Optimize factor exposure calculation batch operations
- [ ] Review and improve caching strategies
- [ ] Consolidate overlapping market data fetches

### 3.1 ProForma Modeling APIs
- [ ] **POST /api/v1/modeling/sessions** - Create modeling session
- [ ] **GET /api/v1/modeling/sessions/{id}** - Get session state
- [ ] **POST /api/v1/modeling/sessions/{id}/trades** - Add ProForma trades
- [ ] **POST /api/v1/modeling/sessions/{id}/calculate** - Calculate impacts
- [ ] **GET /api/v1/modeling/sessions/{id}/impacts** - Get risk impacts
- [ ] **POST /api/v1/modeling/sessions/{id}/save** - Save as snapshot
- [ ] Implement session state management
- [ ] Add trade generation suggestions

### 3.2 Customer Portfolio CSV Upload & Onboarding Workflow
*Complete workflow from CSV upload to batch-processing readiness*

- [ ] **CSV Upload & Validation**
  - [ ] **POST /api/v1/portfolio/upload** - CSV upload endpoint with file validation
    - [ ] Validate CSV format, headers, and data types
    - [ ] Parse OCC options symbols into components (underlying, strike, expiry)
    - [ ] Detect position types automatically (LONG/SHORT for stocks, LC/LP/SC/SP for options)
    - [ ] Validate required fields: ticker, quantity, entry_price, entry_date
    - [ ] Accept optional fields: tags, custom columns (ignored)
    - [ ] Return detailed validation report with row-level errors
  - [ ] **GET /api/v1/portfolio/upload/{id}/status** - Check upload processing status
  - [ ] **GET /api/v1/portfolio/upload/{id}/results** - Get upload results and errors

- [ ] **Security Master Data Enrichment**
  - [ ] **Automatic Security Classification**: For each unique symbol from CSV
    - [ ] Fetch sector, industry, market_cap from Section 1.4.9 providers (FMP/Polygon)
    - [ ] Determine security_type: stock, etf, mutual_fund, option
    - [ ] Collect exchange, country, currency data
    - [ ] Store in market_data_cache with sector/industry fields
    - [ ] Handle symbol validation failures gracefully
  - [ ] **Options Data Processing**: For OCC format symbols
    - [ ] Parse underlying symbol, strike price, expiration date
    - [ ] Validate options chain exists for underlying
    - [ ] Store option-specific fields in position records
    - [ ] Link to underlying security data

- [ ] **Initial Market Data Bootstrap**
  - [ ] **Current Price Fetching**: Bootstrap market data cache
    - [ ] Fetch current prices for all uploaded symbols using Section 1.4.9 providers
    - [ ] Calculate initial market_value using `calculate_position_market_value()`
    - [ ] Calculate initial unrealized_pnl from cost basis
    - [ ] Store baseline prices for batch processing updates
    - [ ] Handle price fetch failures with retry logic
  - [ ] **Options Prerequisites Collection**: For options positions
    - [ ] Fetch implied volatility from options chains
    - [ ] Get risk-free rate from FRED API
    - [ ] Fetch dividend yield for underlying stocks
    - [ ] Store Greeks calculation prerequisites
    - [ ] Enable immediate Batch Job 2 (Greeks) processing

- [ ] **Position Record Creation & Storage**
  - [ ] **Database Population**: Create complete position records
    - [ ] Store all parsed CSV data in positions table
    - [ ] Create portfolio record if new customer
    - [ ] Link positions to portfolio and user accounts
    - [ ] Create tag records for strategy/category labels
    - [ ] Set position metadata: created_at, updated_at
  - [ ] **Data Integrity Validation**: Ensure batch processing prerequisites
    - [ ] Verify all positions have required fields for calculations
    - [ ] Confirm security master data exists for all symbols
    - [ ] Validate market data cache has current prices
    - [ ] Check options positions have complete Greeks prerequisites

- [ ] **Batch Processing Readiness Check**
  - [ ] **POST /api/v1/portfolio/onboarding/{id}/validate** - Validate batch processing readiness
    - [ ] Check Batch Job 1 prerequisites: position records + market data
    - [ ] Check Batch Job 2 prerequisites: options data + Greeks requirements
    - [ ] Check Batch Job 3 prerequisites: security classifications + factor definitions
    - [ ] Return readiness report with any missing data flagged
  - [ ] **POST /api/v1/portfolio/onboarding/{id}/complete** - Complete onboarding process
    - [ ] Trigger initial batch calculations for new portfolio
    - [ ] Generate first portfolio snapshot
    - [ ] Send onboarding completion notification
    - [ ] Enable automatic daily batch processing

- [ ] **Customer Experience Features**
  - [ ] **GET /api/v1/portfolio/onboarding/{id}/preview** - Preview parsed portfolio before confirmation
  - [ ] **POST /api/v1/portfolio/onboarding/{id}/retry** - Retry failed data collection steps
  - [ ] **GET /api/v1/portfolio/templates** - Provide CSV template downloads
  - [ ] Real-time progress updates during onboarding process
  - [ ] Email notifications for onboarding completion/failures

### 3.3 Reporting & Export APIs
- [ ] **POST /api/v1/reports/generate** - Generate reports
- [ ] **GET /api/v1/reports/{id}/status** - Check generation status
- [ ] **GET /api/v1/reports/{id}/download** - Download report
- [ ] **POST /api/v1/export/trades** - Export to FIX/CSV
- [ ] **GET /api/v1/export/history** - Export history
- [ ] Implement async report generation
- [ ] Create export templates

### 3.4 AI Agent Preparation
- [ ] Design async job queue for long-running operations
- [ ] Implement comprehensive error responses
- [ ] Add detailed operation status endpoints
- [ ] Create batch operation endpoints
- [ ] Implement proper pagination everywhere
- [ ] Add filtering and search capabilities
- [ ] Document all endpoints with OpenAPI schemas

### 3.5 Performance Optimization
- [ ] Implement in-memory caching for frequently accessed data
- [ ] Add database query optimization
- [ ] Implement connection pooling
- [ ] Add response compression
- [ ] Profile and optimize critical paths
- [ ] Add database indexes based on query patterns

## Phase 5: Testing & Deployment (Week 7)

### 5.1 Testing
- [ ] Write unit tests for all services
- [ ] Create integration tests for API endpoints
- [ ] Add performance tests for critical operations
- [ ] Test CSV upload with various formats
- [ ] Test authentication flows
- [ ] Create API documentation with examples

### 5.2 Frontend Integration
- [ ] Test with deployed Next.js prototype
- [ ] Adjust API responses to match frontend expectations
- [ ] Implement any missing endpoints discovered during integration
- [ ] Add proper CORS configuration
- [ ] Optimize response formats for frontend consumption

### 5.3 Railway Deployment
- [ ] Create railway.json configuration
- [ ] Set up PostgreSQL on Railway
- [ ] Configure environment variables
- [ ] Configure application for production
- [ ] Deploy FastAPI application
- [ ] Configure custom domain (if needed)
- [ ] Set up monitoring and logging

### 5.4 Documentation
- [ ] Create comprehensive README
- [ ] Document all API endpoints
- [ ] Create deployment guide
- [ ] Write development setup guide
- [ ] Document data models and schemas
- [ ] Create troubleshooting guide

## Phase 6: Demo Preparation (Week 8)

### 6.1 Demo Data Quality
- [ ] Generate realistic 90-day portfolio history
- [ ] Create compelling demo scenarios
- [ ] Ensure smooth user flows
- [ ] Pre-calculate all analytics for demo period
- [ ] Test all demo script scenarios

### 6.2 Performance Tuning
- [ ] Ensure all API responses < 200ms
- [ ] Optimize database queries
- [ ] Cache all demo data
- [ ] Load test with expected demo traffic
- [ ] Fix any performance bottlenecks

### 6.3 Polish & Bug Fixes
- [ ] Fix any frontend integration issues
- [ ] Polish error messages
- [ ] Ensure consistent API responses
- [ ] Add helpful demo tooltips/guides
- [ ] Create demo reset functionality

## Future Enhancements (Post-Demo)

### Data Quality & Calculation Improvements
*Identified during Phase 2.2 factor analysis debug - non-critical but valuable*

#### Factor Analysis Enhancements
- [ ] **Fix SIZE vs SLY ETF inconsistency**
  - `FACTOR_ETFS` uses "SIZE" in `app/constants/factors.py`
  - Backfill list uses "SLY" in `app/batch/market_data_sync.py`
  - Harmonize across codebase to prevent data gaps
  - Verify `FactorDefinition.etf_proxy` matches consistently

- [ ] **Add regression diagnostics logging**
  - Log R² and p-values for each factor regression
  - Detect degenerate cases (near-zero variance)
  - Add warnings for low statistical significance
  - Store regression quality metrics in database

- [ ] **Implement factor correlation matrix**
  - Calculate and store factor correlations
  - Detect multicollinearity issues
  - Warn when factors are highly correlated (>0.8)
  - Use for stress testing and risk analysis

- [ ] **Reconcile 7 vs 8 factor count**
  - Constants define 7 factors with ETF proxies
  - Database has 8 factors (includes "Short Interest" without ETF)
  - Either add 8th ETF proxy or remove from active factors
  - Ensure consistency across seeds, constants, and calculations

#### Calculation Engine Robustness
- [ ] **Improve upsert logic for all calculation engines**
  - Current fix uses existence check + update/insert pattern
  - Consider using PostgreSQL `ON CONFLICT` for atomic upserts
  - Reduce database round trips and improve performance

- [ ] **Add comprehensive calculation diagnostics**
  - Log input data quality (missing values, date gaps)
  - Track calculation duration and resource usage
  - Create calculation audit trail for debugging
  - Add data lineage tracking

- [ ] **Enhance error recovery**
  - Implement partial failure recovery (continue with available data)
  - Add retry logic for transient failures
  - Better error categorization and reporting
  - Create fallback calculations for missing data

### Backlog Items
- [ ] WebSocket support for real-time updates
- [ ] Advanced options pricing models
- [ ] Real-time market data integration
- [ ] Multi-tenant architecture
- [ ] Advanced authentication (OAuth, SSO)
- [ ] Audit logging system
- [ ] Real factor model integration
- [ ] Production-grade monitoring
- [ ] API rate limiting per user
- [ ] Advanced caching strategies

## Development Guidelines

### Code Quality
- Use type hints throughout
- Follow PEP 8 style guide
- Write docstrings for all functions
- Implement proper error handling
- Use async/await for all I/O operations

### Git Workflow
- Create feature branches for each task
- Write descriptive commit messages
- Create PRs for code review
- Tag releases with semantic versioning

### Testing Strategy
- Maintain >80% code coverage
- Test all edge cases
- Use pytest for all tests
- Mock external services in tests

### Security Considerations
- Never commit secrets
- Use environment variables
- Implement input validation
- Sanitize all user inputs
- Use parameterized queries

## Resources

### Documentation
- [API Specifications](./sigmasight-BE/docs/requirements/API_SPECIFICATIONS_V1.4.md)
- [Database Design](./sigmasight-BE/docs/requirements/DATABASE_DESIGN_V1.4.md)
- [Demo Script](./sigmasight-BE/docs/requirements/DEMO_SCRIPT_V1.4.md)
- [PRD](./sigmasight-BE/docs/requirements/PRD_V1.4.md)
- [V5 Prototype Features](./sigmasight-BE/docs/requirements/V0_V5_PROTOTYPE_FEATURES.md)

### External Services
- [Polygon.io API Docs](https://polygon.io/docs)
- [YFinance Documentation](https://pypi.org/project/yfinance/)
- [Railway Deployment Guide](https://docs.railway.app/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)

### Legacy Scripts
- Request legacy Polygon.io integration scripts from PM
- Request legacy GICS data fetching examples

---

## Recent Updates

### Redis & Celery Dependency Cleanup ✅ **COMPLETED** (2025-08-16)
- **Removed unused dependencies**: `redis>=5.0.0` and `celery>=5.3.0` from `pyproject.toml`
- **Cleaned configuration**: Removed `REDIS_URL` from `app/config.py` and `.env.example`
- **Updated documentation**: Removed Redis references from README.md, MAC_INSTALL_GUIDE.md, and TODO2.md
- **Environment cleanup**: `uv sync` removed 12 packages (redis, celery, and dependencies)
- **Simplified architecture**: Application now requires only PostgreSQL database for full functionality

**Impact**: Cleaner codebase, reduced complexity, faster installation, and elimination of unused infrastructure dependencies.

---

**Timeline**: 8 weeks to demo-ready deployment
**Team Size**: 1-2 developers recommended
**Priority**: Phase 1 completion enables basic demo functionality