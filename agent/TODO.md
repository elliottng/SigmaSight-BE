# SigmaSight Agent Implementation TODO

**Created:** 2025-08-27  
**Status:** Planning  
**Target Completion:** 2 weeks  

---

## 🚨 AUTONOMOUS DEVELOPMENT GUIDELINES

### Things Requiring Explicit User Help
**ALWAYS ASK THE USER TO:**
1. **Environment Setup**
   - Add/update API keys in `.env` file (OpenAI, Polygon, FMP, FRED)
   - Launch Docker Desktop application before running PostgreSQL
   - Create accounts or obtain credentials for external services
   - Configure production environment variables

2. **External Dependencies**
   - Install system-level dependencies (PostgreSQL, Redis, etc.)
   - Set up cloud services (AWS, GCP, etc.)
   - Configure DNS or domain settings
   - Set up monitoring/alerting services

3. **Manual Verification**
   - Verify API keys are working with external services
   - Check Docker containers are running properly
   - Confirm database connections after setup
   - Validate production deployment settings

### Things Requiring Explicit Permission
**NEVER DO WITHOUT APPROVAL:**

1. **Database Changes**
   - ❌ Modifying existing backend tables (users, portfolios, positions)
   - ❌ Changing column types or constraints on existing tables
   - ❌ Deleting or renaming existing columns
   - ❌ Creating database changes without Alembic migrations
   - ✅ OK: Creating new agent_* prefixed tables via Alembic
   - ✅ OK: Adding indexes to agent_* tables

2. **API Contract Changes**
   - ❌ Changing existing endpoint paths or methods
   - ❌ Modifying existing Pydantic model fields in backend/app/schemas/
   - ❌ Removing or renaming response fields
   - ❌ Changing authentication requirements
   - ✅ OK: Adding optional parameters with defaults
   - ✅ OK: Creating new endpoints under /api/v1/chat/

3. **Authentication & Security**
   - ❌ Modifying JWT token generation or validation
   - ❌ Changing password hashing algorithms
   - ❌ Altering CORS or security headers
   - ❌ Modifying rate limiting rules
   - ✅ OK: Using existing auth dependencies as-is

4. **Configuration & Environment**
   - ❌ Changing production configuration values
   - ❌ Modifying logging levels in production
   - ❌ Altering cache TTLs without testing
   - ❌ Changing external API rate limits
   - ✅ OK: Adding new AGENT_* prefixed settings

5. **External Service Integration**
   - ❌ Adding new paid API dependencies
   - ❌ Changing API provider (e.g., OpenAI to Anthropic)
   - ❌ Modifying external API usage patterns that increase costs
   - ✅ OK: Using already configured services (OpenAI with existing key)

6. **Data Operations**
   - ❌ Deleting any user data
   - ❌ Running data migrations on existing tables
   - ❌ Modifying data retention policies
   - ❌ Changing backup strategies
   - ✅ OK: Reading data via existing APIs

7. **Performance-Critical Changes**
   - ❌ Modifying database connection pooling
   - ❌ Changing query optimization strategies
   - ❌ Altering caching mechanisms
   - ✅ OK: Adding caching to new agent endpoints

8. **Architectural Decisions**
   - ❌ Changing service boundaries
   - ❌ Modifying the Agent/Backend separation
   - ❌ Altering the communication protocol (REST/SSE)
   - ✅ OK: Following established patterns

### Decision Trees for Common Scenarios

**When You Encounter an Import Error:**
```
IF module not found:
  → Check PYTHONPATH includes /backend
  → Run diagnostic: `PYTHONPATH=/path/to/backend uv run python -c "from app.models.users import User"`
  → If fails: Document error in TODO.md and continue with other tasks
ELSE IF circular import:
  → Move import inside function
  → Use TYPE_CHECKING pattern
```

**When You Get a Database Error:**
```
IF table doesn't exist:
  → Check if migration was created
  → Run: `uv run alembic history` to see migrations
  → Run: `uv run alembic upgrade head`
  → If still fails: Mark task as blocked, document issue
ELSE IF permission denied:
  → Ask user to check Docker is running
  → Verify DATABASE_URL in .env
```

**When OpenAI API Returns Error:**
```
IF 401 Unauthorized:
  → Ask user to verify OPENAI_API_KEY in .env
  → Cannot proceed without valid key
ELSE IF 429 Rate Limited:
  → Implement exponential backoff (max 3 retries)
  → Start with 1s, then 2s, then 4s delay
ELSE IF 500+ Server Error:
  → Log error and return graceful message to user
  → Switch to fallback model if configured
```

---

## 📚 Requirements Documents Cross-Reference

### Primary Specifications
- **[PRD_AGENT_V1.0.md](../_docs/requirements/PRD_AGENT_V1.0.md)** - Product requirements, user flows, success metrics
- **[DESIGN_DOC_AGENT_V1.0.md](../_docs/requirements/DESIGN_DOC_AGENT_V1.0.md)** - Technical design, architecture, existing infrastructure (Section 18)
- **[DESIGN_DOC_FRONTEND_V1.0.md](../../frontend/_docs/requirements/DESIGN_DOC_FRONTEND_V1.0.md)** - Frontend specs (Phase 2)

### Backend Context
- **[API_IMPLEMENTATION_STATUS.md](../../backend/API_IMPLEMENTATION_STATUS.md)** - Current API completion status (23% overall, 100% Raw Data)
- **[TODO3.md](../../backend/TODO3.md)** - Backend Phase 3 status, UTC datetime standardization ✅
- **[AI_AGENT_REFERENCE.md](../../backend/AI_AGENT_REFERENCE.md)** - Codebase patterns, import paths, common errors

---

## 🎯 Overview

Implement a chat-based portfolio analysis agent that uses OpenAI's API with function calling to Raw Data endpoints and Code Interpreter for calculations. This requires new backend chat endpoints, enhancing Raw Data APIs, and tool handler implementations.

**Architecture Requirement:**
- **SERVICE SEPARATION**: Agent must be implemented as an isolated module that can be cleanly extracted into a standalone microservice. See PRD §3.1-3.2 and TDD §2.1 for separation requirements.

**Critical Issues to Address:**
- ✅ GPT-5 now available - use as default model (per OpenAI docs)
- ✅ Raw Data APIs (6/6) return real data - need parameter enhancements only
- ✅ Backend chat endpoints IMPLEMENTED - /api/v1/chat/* ready
- ✅ UTC ISO 8601 standardization COMPLETED (Phase 3)
- ⚠️ Agent must use HTTP calls to Raw Data APIs (no direct DB access)

---

## 📋 Phase 0: Prerequisites & Fixes (Day 1-2)

> **Status Update (2025-08-28):**
> - ✅ Dual authentication (0.3) - COMPLETED and tested
> - ✅ GPT-5 configuration (0.1) - Model references updated
> - ✅ Environment setup (0.2) - COMPLETED
> - ✅ Database schema (0.4) - COMPLETED with migrations

### 0.1 Configure GPT-5 Model Settings ✅ **COMPLETED**
- [x] **Set up GPT-5 as default model** (ref: PRD §3, TDD §17)
  - [x] 👤 **USER ACTION**: Verify GPT-5 access in OpenAI account
  - [x] Set MODEL_DEFAULT = "gpt-5"
  - [x] Set MODEL_FALLBACK = "gpt-5-mini"
  - [x] Update DESIGN_DOC_AGENT_V1.0.md to confirm GPT-5 usage
  - [x] Update PRD_AGENT_V1.0.md model references
  
  **Success Criteria:**
  - ✅ Config loads without errors: `python -c "from app.config import settings; print(settings.MODEL_DEFAULT)"`
  - ✅ Returns "gpt-5"

### 0.2 Environment Setup ✅ **COMPLETED**
- [x] **Update backend/app/config.py with OpenAI settings**
  
  **File:** `backend/app/config.py`
  **Location:** After line ~45 (after existing settings)
  ```python
  # Add to Settings class (uses pydantic_settings pattern)
  OPENAI_API_KEY: str = Field(..., env="OPENAI_API_KEY")
  OPENAI_ORG_ID: str = Field(default="", env="OPENAI_ORG_ID")  # Optional
  MODEL_DEFAULT: str = Field(default="gpt-5", env="MODEL_DEFAULT")
  MODEL_FALLBACK: str = Field(default="gpt-5-mini", env="MODEL_FALLBACK")
  AGENT_CACHE_TTL: int = Field(default=600, env="AGENT_CACHE_TTL")
  SSE_HEARTBEAT_INTERVAL_MS: int = Field(default=15000, env="SSE_HEARTBEAT_INTERVAL_MS")
  ```

- [x] 👤 **USER ACTION: Add to .env file** ✅ **COMPLETED**
  ```bash
  OPENAI_API_KEY=sk-...  # User must provide
  OPENAI_ORG_ID=org-... (if applicable)
  MODEL_DEFAULT=gpt-5
  MODEL_FALLBACK=gpt-5-mini
  AGENT_CACHE_TTL=600
  SSE_HEARTBEAT_INTERVAL_MS=15000
  ```
  
  **Validation:**
  ```bash
  cd backend
  uv run python -c "from app.config import settings; assert settings.OPENAI_API_KEY.startswith('sk-'), 'API key not set'"
  ```
  
  **If validation fails:** Ask user to update .env file with OpenAI API key

### 0.3 Implement Dual Authentication Support ✅ **COMPLETED**
> **See canonical implementation**: `backend/TODO3.md` Section 4.0.1 - Dual Authentication Strategy
> Implemented 2025-08-27 - Both Bearer tokens and cookies are now supported!

- [x] **Summary**: Implemented dual auth (Bearer + Cookie) per backend/TODO3.md §4.0.1
  - [x] Bearer tokens work for all REST APIs (preferred method)
  - [x] Cookies work as fallback (required for SSE)
  - [x] No breaking changes - both methods fully supported and tested

### 0.4 Database Schema Updates (via Alembic Migrations) ✅ **COMPLETED**
- [x] **Create Agent-specific SQLAlchemy models** (ref: TDD §18.2 for patterns)
  
  **Step 1: Create directory structure**
  ```bash
  mkdir -p backend/app/agent/models
  touch backend/app/agent/models/__init__.py
  touch backend/app/agent/models/conversations.py
  touch backend/app/agent/models/preferences.py
  ```
  
  **Step 2: Create conversations.py**
  - [x] File: `backend/app/agent/models/conversations.py` ✅
  - [x] Define `Conversation` model class (agent_conversations table) ✅
  - [x] Define `Message` model class (agent_messages table) ✅
  - [ ] Import from: `from app.database import Base`
  - [ ] Use UUID primary keys: `from uuid import uuid4`
  
  **Step 3: Create preferences.py**
  - [x] File: `backend/app/agent/models/preferences.py` ✅
  - [x] Define `UserPreference` model (agent_user_preferences table) ✅
  
  **Success Criteria:**
  - ✅ Models import without error: `python -c "from app.agent.models.conversations import Conversation"`
  - ✅ All tables have agent_ prefix
  - ✅ All models inherit from Base

- [x] **Update Alembic configuration** ✅
  - [x] Import Agent models in `backend/alembic/env.py`: ✅
    ```python
    from app.agent.models import conversations, preferences
    ```
  - [x] Ensure Agent models are included in autogenerate ✅

- [x] **Create and run Alembic migration** ✅ **COMPLETED**
  ```bash
  cd backend
  # Create migration
  uv run alembic revision --autogenerate -m "Create Agent tables (conversations, messages, preferences)"
  
  # Review generated migration file
  # Ensure all tables have agent_ prefix
  
  # Apply migration
  uv run alembic upgrade head
  
  # Verify tables created
  uv run python -c "from app.database import engine; print(engine.table_names())"
  ```

- [x] **Conversation model schema** (Agent owns these tables!) ✅ **COMPLETED**
  ```python
  class Conversation(Base):
      __tablename__ = "agent_conversations"  # Note: agent_ prefix
      
      id = Column(UUID, primary_key=True, default=uuid4)  # Our canonical ID, returned as conversation_id
      user_id = Column(UUID, nullable=False)  # Reference to users.id but NO FK (clean separation)
      mode = Column(String(50), default="green")
      
      # Provider tracking (vendor-agnostic)
      provider = Column(String(32), default="openai")
      provider_thread_id = Column(String(255), nullable=True)  # OpenAI thread ID if using Assistants
      provider_run_id = Column(String(255), nullable=True)     # OpenAI run ID if applicable
      
      created_at = Column(DateTime, default=utc_now)
      updated_at = Column(DateTime, default=utc_now, onupdate=utc_now)
      metadata = Column(JSONB, default={})  # For model version, settings, etc.
      
      # Relationships
      user = relationship("User", back_populates="conversations")
      messages = relationship("ConversationMessage", back_populates="conversation")
      
      # Indexes
      __table_args__ = (
          Index("idx_conversations_user_id", "user_id"),
          Index("idx_conversations_created_at", "created_at"),
          Index("idx_conversations_provider_thread_id", "provider_thread_id"),  # Non-unique for lookups
      )
  ```

- [x] **ConversationMessage model schema** ✅ **COMPLETED**
  ```python
  class ConversationMessage(Base):
      __tablename__ = "conversation_messages"
      
      id = Column(UUID, primary_key=True, default=uuid4)
      conversation_id = Column(UUID, ForeignKey("conversations.id"))
      role = Column(String(50), nullable=False)  # 'user', 'assistant', 'system', 'tool'
      content = Column(Text, nullable=True)  # Can be null for tool-only responses
      tool_calls = Column(JSONB, default=[])
      
      # Performance metrics
      first_token_ms = Column(Integer, nullable=True)  # Time to first SSE token (critical metric!)
      latency_ms = Column(Integer, nullable=True)      # Total response time
      
      # Token tracking
      prompt_tokens = Column(Integer, nullable=True)
      completion_tokens = Column(Integer, nullable=True)
      total_tokens = Column(Integer, nullable=True)
      
      # Provider tracking
      provider_message_id = Column(String(255), nullable=True)  # OpenAI message ID for debugging
      
      created_at = Column(DateTime, default=utc_now)
      error = Column(JSONB, nullable=True)
      
      # Relationships
      conversation = relationship("Conversation", back_populates="messages")
      
      # Indexes
      __table_args__ = (
          Index("idx_messages_conversation_id", "conversation_id"),
          Index("idx_messages_created_at", "created_at"),
      )
  ```

- [x] **Generate and apply Alembic migration** ✅ **COMPLETED**
  
  **Prerequisites:**
  - [x] 👤 **USER ACTION**: Ensure Docker Desktop is running ✅
  - [x] 👤 **USER ACTION**: Ensure PostgreSQL container is up: `docker-compose up -d` ✅
  
  **Step 1: Generate migration**
  ```bash
  cd backend
  uv run alembic revision --autogenerate -m "Add conversation tables for agent"
  ```
  
  **Step 2: Review migration**
  - [ ] Check file in `backend/alembic/versions/`
  - [ ] Verify all tables have agent_ prefix
  - [ ] Verify indexes are created
  
  **Step 3: Test migration (dry run)**
  ```bash
  uv run alembic upgrade head --sql > migration_preview.sql
  cat migration_preview.sql  # Review SQL
  ```
  
  **Step 4: Apply migration**
  ```bash
  uv run alembic upgrade head
  ```
  
  **Success Criteria:**
  - ✅ Migration applies without errors
  - ✅ Tables exist in database:
    ```bash
    uv run python -c "from app.database import engine; import asyncio; asyncio.run(engine.execute('SELECT tablename FROM pg_tables WHERE tablename LIKE \'agent_%\''))"
    ```
  
  **Rollback if needed:**
  ```bash
  uv run alembic downgrade -1
  ```
  
  **Step 5: Update database initialization**
  - [x] File: `backend/app/database.py` ✅
  - [x] Location: Around line 85 in init_db() ✅
  - [x] Add: `from app.agent.models import conversations, preferences` ✅ **Models imported in alembic/env.py**

- [ ] **Data retention considerations (for production)**
  - [ ] Plan for 30-60 day retention policy to prevent unbounded growth
  - [ ] Consider truncating large tool outputs (store preview only)
  - [ ] Note: Skip PII redaction for prototype phase


### 🎯 Architecture Benefits Summary

**Provider Portability (95% Code Reuse):**
- ✅ **Business logic layer**: 100% portable (data fetching, filtering, caps, meta objects)
- ✅ **Response formatting**: 100% portable (common envelope, error handling)
- 🔧 **Provider adapters**: Only 5% provider-specific (schema formats, response conversion)
- 🚀 **Migration cost**: 1-2 days per new provider vs complete rebuild

**Phase 1 Delivery:**
- 🎯 **Focus**: OpenAI adapter implementation only
- 🏗️ **Structure**: Business logic designed for portability from day one
- 🔮 **Future**: Ready for Anthropic, Gemini, Grok with minimal effort

---

## 🏗️ Service Separation Architecture (Throughout All Phases)

### Isolation Requirements
- [x] **Create isolated Agent module structure** ✅ **PARTIAL - Core structure created**
  ```
  backend/app/agent/
  ├── __init__.py
  ├── config.py           # Agent-specific settings (AGENT_ prefix)
  ├── router.py           # FastAPI router for /api/v1/chat/*
  ├── handlers/           # Request handlers
  ├── tools/              # Tool implementations
  ├── clients/            # HTTP client for Raw Data APIs
  ├── models.py           # Agent-specific Pydantic models
  └── logging.py          # Agent-specific logger
  ```

### Development Rules
- [x] **Agent owns its database schema** ✅ **COMPLETED**
  - [x] Create Agent SQLAlchemy models in `app/agent/models/` ✅
  - [x] Use `agent_` prefix for all Agent tables ✅
  - [x] **ALWAYS use Alembic migrations** (never create tables manually) ✅
  - [x] Direct database access for Agent tables (conversations, messages, etc.) ✅
  - [ ] NO access to backend tables (users, portfolios, positions)
  - [ ] Use HTTP client for ALL portfolio/market data

- [ ] **Independent configuration**
  - [ ] Create `AgentSettings` class with `AGENT_` prefix
  - [ ] Separate OpenAI keys and settings
  - [ ] Injectable backend API base URL

- [ ] **HTTP-only communication**
  - [ ] Create `RawDataClient` class using httpx 🔶 **DEFERRED - Using service layer pattern instead**
  - [ ] Include auth headers in all requests
  - [ ] Handle retries and timeouts

- [ ] **Testing isolation**
  - [ ] Unit tests mock all Raw Data API responses
  - [ ] Integration tests use actual HTTP calls
  - [ ] No database fixtures in Agent tests

---

## 📋 Phase 1: Enhance Data API Endpoints for Agent Use (Day 2-3) ✅ **COMPLETED**

> **Status Update (2025-08-28):**
> - ✅ `/data/positions/top/{portfolio_id}` - COMPLETED with all specs
> - ❌ `/data/portfolio/{id}/summary` - REMOVED (requires performance calculations that don't exist)
> - ✅ `/data/portfolio/{id}/complete` - ENHANCED with include flags and meta object

> **ARCHITECTURE UPDATE**: Based on review feedback, we're enhancing existing data endpoints
> with agent-optimized parameters rather than having tool handlers apply business logic.
> 
> Enhanced endpoints at `/api/v1/data/*` will handle:
> - Symbol selection logic (top N by value/weight)
> - Token-aware response sizing
> - Pre-filtered, capped responses
> 
> Reference: TDD §7.0 for architectural decision, §7.1-7.6 for tool specifications

### 1.0 NEW Backend Components Required ✅ **COMPLETED**

> **Note**: Current endpoints query DB directly in API layer. For new Agent features,
> we need both Pydantic schemas and a service layer.

- [x] **Create `app/schemas/data.py`** - Pydantic schemas for data endpoints ✅
  
  **File:** `backend/app/schemas/data.py`
  ```python
  from pydantic import BaseModel, Field
  from typing import Dict, List, Optional, Any
  from datetime import datetime
  from uuid import UUID
  from app.schemas.base import BaseSchema
  
  class MetaInfo(BaseModel):
      """Common meta object for all agent responses"""
      as_of: datetime
      requested: Dict[str, Any]
      applied: Dict[str, Any]
      limits: Dict[str, int]
      rows_returned: int
      truncated: bool = False
      suggested_params: Optional[Dict[str, Any]] = None
  
  class PositionSummary(BaseSchema):
      position_id: UUID
      symbol: str
      quantity: float
      market_value: float
      weight: float
      pnl_dollar: float
      pnl_percent: float
  
  class TopPositionsResponse(BaseSchema):
      meta: MetaInfo
      positions: List[PositionSummary]
      portfolio_coverage: float  # % of portfolio value covered
  
  class PortfolioSummaryResponse(BaseSchema):
      meta: MetaInfo
      portfolio_id: UUID
      total_value: float
      cash_balance: float
      positions_count: int
      top_holdings: List[PositionSummary]
  ```
  
  **Success Criteria:**
  - ✅ Schemas import without error: `python -c "from app.schemas.data import MetaInfo"`
  - ✅ All schemas inherit from BaseSchema
  - ✅ Meta object follows TDD §7.A spec

- [x] **Create `app/services/portfolio_data_service.py`** ✅ **COMPLETED**
  
  **File:** `backend/app/services/portfolio_data_service.py`
  ```python
  from sqlalchemy.ext.asyncio import AsyncSession
  from sqlalchemy import select, func, desc
  from uuid import UUID
  from typing import List, Dict, Any, Optional
  from app.models.users import Portfolio
  from app.models.positions import Position
  from app.models.market_data import MarketDataCache
  from app.schemas.data import TopPositionsResponse, PortfolioSummaryResponse, PositionSummary, MetaInfo
  from app.core.datetime_utils import utc_now
  
  class PortfolioDataService:
      """Service layer for Agent-optimized portfolio data operations"""
      
      async def get_top_positions_by_value(
          self,
          db: AsyncSession,
          portfolio_id: UUID,
          limit: int = 50
      ) -> TopPositionsResponse:
          """Get top N positions by market value"""
          # Implementation here
          pass
      
      async def get_portfolio_summary(
          self,
          db: AsyncSession,
          portfolio_id: UUID
      ) -> PortfolioSummaryResponse:
          """Get condensed portfolio overview"""
          # Implementation here
          pass
      
      async def get_historical_prices_with_selection(
          self,
          db: AsyncSession,
          portfolio_id: UUID,
          selection_method: str = "top_by_value",
          max_symbols: int = 5
      ) -> Dict[str, Any]:
          """Get historical prices for selected symbols"""
          # Implementation here
          pass
  ```
  
  **Success Criteria:**
  - ✅ Service imports without error
  - ✅ All methods are async
  - ✅ All methods return proper response types
  
  **Testing:**
  ```bash
  uv run pytest tests/test_portfolio_data_service.py -v
  ```

### 1.1 Priority New Endpoints (LLM-Optimized)

- [x] **GET /api/v1/data/positions/top/{portfolio_id}** - New endpoint ✅ **COMPLETED**
  
  **API Layer Responsibilities:**
  - [x] Sorting by market value/weight ✅
  - [x] Computing portfolio coverage percentage ✅
  - [x] Applying limit caps: `limit<=50`, `as_of_date<=180d` lookback ✅
  - [x] Response shape: `{symbol, name, qty, value, weight, sector}` only ✅
  - [x] Round weight to 4 decimal places ✅
  - [x] Full meta object: `requested/applied/as_of/truncated/limits/schema_version` ✅
  
  **File:** `backend/app/api/v1/data.py`
  ```python
  @router.get("/positions/top/{portfolio_id}")
  async def get_top_positions(
      portfolio_id: UUID,
      limit: int = Query(20, le=50, description="Max positions to return"),
      sort_by: str = Query("market_value", regex="^(market_value|weight)$"),
      as_of_date: Optional[str] = Query(None, description="ISO date, max 180d lookback"),
      service: PortfolioDataService = Depends(get_portfolio_data_service),
      current_user: CurrentUser = Depends(get_current_user),
      db: AsyncSession = Depends(get_async_session)
  ):
      return await service.get_top_positions(
          db, portfolio_id, limit, sort_by, as_of_date
      )
  ```
  
  **Service Implementation:**
  ```python
  # In PortfolioDataService
  async def get_top_positions(
      self,
      db: AsyncSession, 
      portfolio_id: UUID,
      limit: int = 20,
      sort_by: str = "market_value",
      as_of_date: Optional[str] = None
  ) -> Dict:
      # 1. Query positions with market values
      # 2. Sort by market_value or weight 
      # 3. Apply limit cap (<=50)
      # 4. Calculate portfolio coverage %
      # 5. Format response with proper meta object
      # 6. Round weight to 4dp
  ```
  
  **Handler Layer (Ultra-Thin):**
  - [x] Validate inputs with default `limit=20` ✅
  - [x] Call API endpoint ✅
  - [x] Wrap in uniform envelope ✅
  - [x] Map transient errors to `retryable=true` ✅


### 1.2 Existing Endpoint Enhancements

- [x] **GET /api/v1/data/portfolio/{portfolio_id}/complete** - Add include flags ✅ **COMPLETED**
  - ✅ Returns real portfolio data with positions
  - ✅ cash_balance calculated as 5% of portfolio
  
  **API Layer Enhancements:**
  - [x] Add `include_holdings` boolean parameter (default: true) ✅
  - [x] Add `include_timeseries` boolean parameter (default: false) ✅
  - [x] Add `include_attrib` boolean parameter (default: false) ✅
  - [x] Provide consistent `as_of` timestamp across all sections ✅
  - [x] Deterministic ordering of positions/data ✅
  - [x] Full meta object population ✅
  
  **Enhanced endpoint signature:**
  ```python
  @router.get("/portfolio/{portfolio_id}/complete")
  async def get_portfolio_complete(
      portfolio_id: UUID,
      include_holdings: bool = Query(True, description="Include position details"),
      include_timeseries: bool = Query(False, description="Include historical data"),
      include_attrib: bool = Query(False, description="Include attribution data"), 
      service: PortfolioDataService = Depends(get_portfolio_data_service),
      current_user: CurrentUser = Depends(get_current_user),
      db: AsyncSession = Depends(get_async_session)
  ):
      return await service.get_portfolio_complete(
          db, portfolio_id, include_holdings, include_timeseries, include_attrib
      )
  ```
  
  **Handler Layer (Ultra-Thin):**
  - [ ] Validate inputs only
  - [ ] Call API endpoint  
  - [ ] Wrap in uniform envelope
  - [ ] No truncation note logic (that belongs in API layer)

- [x] **GET /api/v1/data/portfolio/{portfolio_id}/data-quality**
  - ✅ Returns real data quality assessment
  - [ ] Add `check_factors` boolean parameter (default: true)
  - [ ] Add `check_correlations` boolean parameter (default: true)
  - [ ] Enhance feasibility flags for downstream analytics

### 1.2 Position Data Endpoints ✅ WORKING - Need Enhancements
- [x] **GET /api/v1/data/positions/details**
  - ✅ Returns real positions from database
  - [ ] Add support for position_ids comma-separated list (currently portfolio_id only)
  - [ ] Add `include_closed` boolean parameter (default: false)
  - [ ] Enforce max_rows=200 cap with truncation metadata
  - [ ] Add summary block to response
  - [ ] Return meta object with truncation info

### 1.3 Price Data Endpoints ✅ WORKING - Need Enhancements
- [x] **GET /api/v1/data/prices/historical/{portfolio_id}**
  - ✅ Returns 292 days of real OHLCV data from MarketDataCache
  - [ ] Add `lookback_days` parameter with max=180 enforcement
  - [ ] Add `include_factor_etfs` boolean parameter (default: true)
  - [ ] Add `date_format` parameter (iso/unix, default: iso)
  - [ ] Return meta object with applied limits

- [x] **GET /api/v1/data/prices/quotes**
  - ✅ Returns real-time quotes with volume
  - [ ] Add max_symbols=5 cap enforcement
  - [ ] Add `include_options` boolean parameter (default: false)
  - [ ] Handle invalid symbols gracefully
  - [ ] Add 60-second cache TTL

### 1.4 Factor Data Endpoints ✅ WORKING - Need Enhancements
- [x] **GET /api/v1/data/factors/etf-prices**
  - ✅ All 7 ETFs return real market prices
  - [ ] Add `lookback_days` parameter (default: 150)
  - [ ] Add `factors` parameter for filtering (comma-separated)
  - [ ] Map factor names to ETF symbols (e.g., "market" → "SPY")
  - [ ] Return meta object with resolved symbols

### 1.5 Testing & Validation
- [x] **Test scripts already exist**
  - ✅ `scripts/verify_mock_vs_real_data.py` - Confirms all return real data
  - ✅ `scripts/check_etf_mapping.py` - Verifies ETF data
  - ✅ `scripts/test_historical_prices.py` - Validates price data
  - [ ] Add tests for new parameters
  - [ ] Verify meta object format
  - [ ] Test truncation behavior

---

## 📋 Phase 2: Backend Chat Infrastructure (Day 4-6)

> Reference: TDD §5 (Chat Endpoints), §8 (SSE Protocol), §18.1 (Auth), §18.4 (API Structure)

### 2.0 Agent Pydantic Schemas ✅ **COMPLETED**

- [x] **Create `app/agent/schemas/` directory** ✅
  - [x] `__init__.py` - Export all schemas ✅
  - [x] `base.py` - AgentBaseSchema with common config ✅
  
- [x] **Create `app/agent/schemas/chat.py`** ✅
  ```python
  from app.agent.schemas.base import AgentBaseSchema
  
  class ConversationCreate(AgentBaseSchema):
      mode: str = "green"  # green|blue|indigo|violet
      
  class ConversationResponse(AgentBaseSchema):
      conversation_id: UUID
      mode: str
      created_at: datetime
      
  class MessageSend(AgentBaseSchema):
      conversation_id: UUID
      text: str
  ```

- [x] **Create `app/agent/schemas/sse.py`** ✅ **COMPLETED**
  ```python
  class SSEEvent(AgentBaseSchema):
      event: str  # start|message|tool_call|tool_result|error|done
      data: Dict
      
  class ToolCallEvent(AgentBaseSchema):
      name: str
      args: Dict
      
  class ToolResultEvent(AgentBaseSchema):
      name: str
      result: Dict
      meta: Dict
  ```

### 2.1 Create Chat Module Structure ✅ **COMPLETED**
- [x] **Create backend/app/api/v1/chat/ module** (ref: TDD §3 for structure) ✅
  
  **Step 1: Create directory and files**
  ```bash
  mkdir -p backend/app/api/v1/chat
  touch backend/app/api/v1/chat/__init__.py
  touch backend/app/api/v1/chat/router.py
  touch backend/app/api/v1/chat/conversations.py
  touch backend/app/api/v1/chat/send.py
  touch backend/app/api/v1/chat/tools.py
  touch backend/app/api/v1/chat/schemas.py
  ```
  
  **Step 2: Create router.py**
  ```python
  # File: backend/app/api/v1/chat/router.py
  from fastapi import APIRouter
  from .conversations import router as conversations_router
  from .send import router as send_router
  
  router = APIRouter()
  router.include_router(conversations_router)
  router.include_router(send_router)
  ```
  
  **Step 3: Register in main router**
  - [x] File: `backend/app/api/v1/router.py` ✅
  - [x] Add after existing includes (around line 20): ✅
    ```python
    from .chat import router as chat_router
    api_router.include_router(chat_router.router, prefix="/chat", tags=["chat"])
    ```
  
  **Success Criteria:**
  - ✅ Server starts without import errors
  - ✅ /api/v1/chat endpoints appear in /docs
  - ✅ No circular imports

### 2.2 Implement Conversation Management ✅ **COMPLETED**
- [x] **POST /chat/conversations endpoint** (ref: TDD §5.1, PRD §7.1) ✅
  
  **File:** `backend/app/api/v1/chat/conversations.py`
  ```python
  from fastapi import APIRouter, Depends, HTTPException
  from sqlalchemy.ext.asyncio import AsyncSession
  from uuid import uuid4
  from app.database import get_db
  from app.core.dependencies import get_current_user, CurrentUser
  from app.agent.models.conversations import Conversation
  from app.agent.schemas.chat import ConversationCreate, ConversationResponse
  from app.core.datetime_utils import utc_now
  
  router = APIRouter()
  
  @router.post("/conversations", response_model=ConversationResponse)
  async def create_conversation(
      request: ConversationCreate,
      db: AsyncSession = Depends(get_db),
      current_user: CurrentUser = Depends(get_current_user)
  ):
      """Create a new conversation"""
      conversation = Conversation(
          id=uuid4(),  # Our canonical ID
          user_id=current_user.id,
          mode=request.mode or "green",
          provider="openai",
          created_at=utc_now(),
          updated_at=utc_now()
      )
      db.add(conversation)
      await db.commit()
      await db.refresh(conversation)
      
      return ConversationResponse(
          conversation_id=str(conversation.id),
          mode=conversation.mode,
          created_at=conversation.created_at
      )
  ```
  
  **Success Criteria:**
  - ✅ POST /api/v1/chat/conversations returns 201
  - ✅ Returns UUID as conversation_id
  - ✅ Conversation saved to database
  - ✅ Auth required (401 without token)
  
  **Test:**
  ```bash
  curl -X POST "http://localhost:8000/api/v1/chat/conversations" \
    -H "Authorization: Bearer {token}" \
    -H "Content-Type: application/json" \
    -d '{"mode": "green"}'
  ```

- [x] **Conversation schemas** ✅ **COMPLETED**
  ```python
  class ConversationCreate(BaseSchema):
      mode: Optional[str] = "green"  # green|blue|indigo|violet
  
  class ConversationResponse(BaseSchema):
      conversation_id: str
      mode: str
      created_at: datetime
  ```

### 2.3 Implement SSE Streaming Endpoint 🔶 **PARTIAL - Placeholder Implementation**
- [x] **POST /chat/send (SSE)** (ref: TDD §5.2, §8 for SSE protocol, PRD §4.3) ✅ **Placeholder ready for OpenAI integration**
  
  **File:** `backend/app/api/v1/chat/send.py`
  ```python
  from fastapi import APIRouter, Depends, HTTPException, Request
  from fastapi.responses import StreamingResponse
  from sqlalchemy.ext.asyncio import AsyncSession
  import asyncio
  import json
  from typing import AsyncGenerator
  from app.database import get_db
  from app.core.dependencies import get_current_user_sse, CurrentUser
  from app.agent.schemas.chat import MessageSend
  from app.services.openai_service import OpenAIService
  from app.agent.models.conversations import Conversation, ConversationMessage
  from app.core.datetime_utils import utc_now
  from app.core.logging import get_logger
  
  logger = get_logger(__name__)
  router = APIRouter()
  
  async def sse_generator(
      message: str,
      conversation: Conversation,
      openai_service: OpenAIService,
      db: AsyncSession
  ) -> AsyncGenerator[str, None]:
      """Generate SSE events"""
      try:
          # Send start event
          yield f"event: start\ndata: {json.dumps({'mode': conversation.mode})}\n\n"
          
          # Handle mode switching
          if message.startswith("/mode "):
              new_mode = message[6:].strip()
              if new_mode in ["green", "blue", "indigo", "violet"]:
                  conversation.mode = new_mode
                  await db.commit()
                  yield f"event: message\ndata: {json.dumps({'delta': f'Mode changed to {new_mode}'})}\n\n"
                  yield "event: done\ndata: {}\n\n"
                  return
          
          # Stream OpenAI response
          async for chunk in openai_service.stream_completion(message, conversation):
              if chunk.get('type') == 'delta':
                  yield f"event: message\ndata: {json.dumps({'delta': chunk['content']})}\n\n"
              elif chunk.get('type') == 'tool_call':
                  yield f"event: tool_call\ndata: {json.dumps(chunk)}\n\n"
              elif chunk.get('type') == 'tool_result':
                  yield f"event: tool_result\ndata: {json.dumps(chunk)}\n\n"
          
          # Send done event
          yield "event: done\ndata: {}\n\n"
          
      except Exception as e:
          logger.error(f"SSE error: {e}")
          yield f"event: error\ndata: {json.dumps({'message': str(e)})}\n\n"
  
  @router.post("/send")
  async def send_message(
      request: MessageSend,
      db: AsyncSession = Depends(get_db),
      current_user: CurrentUser = Depends(get_current_user_sse)  # Special SSE auth
  ):
      """Send message and stream response via SSE"""
      # Load conversation
      conversation = await db.get(Conversation, request.conversation_id)
      if not conversation or conversation.user_id != current_user.id:
          raise HTTPException(status_code=404, detail="Conversation not found")
      
      # Set up SSE response
      openai_service = OpenAIService()
      generator = sse_generator(request.text, conversation, openai_service, db)
      
      return StreamingResponse(
          generator,
          media_type="text/event-stream",
          headers={
              "Cache-Control": "no-cache",
              "Connection": "keep-alive",
              "X-Accel-Buffering": "no",  # Disable nginx buffering
          }
      )
  ```
  
  **Create SSE auth dependency:**
  - [ ] File: `backend/app/core/dependencies.py`
  - [ ] Add function `get_current_user_sse()` that checks cookie first, then query param
  
  **Success Criteria:**
  - ✅ SSE connection established
  - ✅ Events stream properly formatted
  - ✅ Mode switching works
  - ✅ Errors returned as SSE events
  
  **Test with curl:**
  ```bash
  curl -X POST "http://localhost:8000/api/v1/chat/send" \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer {token}" \
    -d '{"conversation_id": "{uuid}", "text": "What is my portfolio value?"}' \
    -N  # No buffering for SSE
  ```

- [ ] **SSE Contract (Frontend Compatibility)**
  ```python
  # Ensure server emits distinct SSE events:
  
  class SSEEvent:
      START = "start"
      TOOL_STARTED = "tool_started"
      TOOL_DELTA = "tool_delta"         # Optional streaming
      TOOL_FINISHED = "tool_finished"
      CONTENT_DELTA = "content_delta"   # Model text tokens
      HEARTBEAT = "heartbeat"           # Every ~10s
      ERROR = "error"
      DONE = "done"
  
  # SSE generator updates:
  async def sse_generator(...):
      # Send heartbeat every 10s
      last_heartbeat = time.time()
      
      # Tool execution events
      yield f"event: tool_started\ndata: {json.dumps({'name': tool_name, 'args': args})}\n\n"
      
      # Tool completion
      yield f"event: tool_finished\ndata: {json.dumps({'name': tool_name, 'result': envelope})}\n\n"
      
      # Model response streaming
      async for chunk in openai_stream:
          yield f"event: content_delta\ndata: {json.dumps({'delta': chunk})}\n\n"
      
      # Periodic heartbeat
      if time.time() - last_heartbeat > 10:
          yield f"event: heartbeat\ndata: {json.dumps({'ts': utc_now().isoformat()})}\n\n"
          last_heartbeat = time.time()
  
  # Ensure proxy_buffering off is honored
  headers = {
      "Cache-Control": "no-cache",
      "Connection": "keep-alive", 
      "X-Accel-Buffering": "no"  # Critical for real-time
  }
  ```

- [ ] **Request/Response schemas**
  ```python
  class ChatSendRequest(BaseSchema):
      conversation_id: str
      text: str
      
  # Mode switching detection in handler:
  # if text.startswith("/mode "):
  #     new_mode = text[6:].strip()  # green|blue|indigo|violet
  #     update conversation.mode in DB
  #     return SSE event confirming mode change
  
  class SSEMessageEvent(BaseSchema):
      delta: str
  
  class SSEToolCallEvent(BaseSchema):
      name: str
      args: Dict[str, Any]
  
  class SSEToolResultEvent(BaseSchema):
      name: str
      meta: Dict[str, Any]
      preview: Optional[Dict[str, Any]]
  ```

### 2.4 OpenAI Integration
- [ ] **Create OpenAI service module**
  ```python
  backend/app/services/openai_service.py
  ```
  - [ ] Initialize OpenAI client with API key
  - [ ] Implement conversation creation
  - [ ] Implement message sending with streaming
  - [ ] Handle tool calls
  - [ ] Enable Code Interpreter

- [ ] **Error handling**
  - [ ] Rate limit handling with retry
  - [ ] Token limit management (GPT-5 has higher limits)
  - [ ] Connection error recovery
  - [ ] Graceful degradation
  - [ ] Handle GPT-5 specific response formats

---

## 📋 Phase 3: Provider-Agnostic Tool Handlers (Day 6-8)

> Reference: TDD §7.0 (Provider-Agnostic Tool Architecture), PRD §6 (Tool Schemas)
> 
> **Architecture Note**: Structured for multi-provider support (OpenAI, Anthropic, Gemini, Grok)
> with 95% code reuse. Phase 1 implements OpenAI adapter only.

### 3.1 Tool Registry + Ultra-Thin Handlers
- [ ] **Create `backend/app/agent/tools/tool_registry.py`**
  ```python
  from typing import Dict, Callable, Any
  from pydantic import BaseModel, ValidationError
  
  # Registry of all available tools
  TOOL_REGISTRY: Dict[str, Callable] = {
      "get_portfolio_complete": get_portfolio_complete,
      "get_positions_details": get_positions_details,
      "get_prices_historical": get_prices_historical,
      "get_current_quotes": get_current_quotes,
      "get_factor_etf_prices": get_factor_etf_prices,
      "get_portfolio_data_quality": get_portfolio_data_quality
  }
  
  async def dispatch_tool_call(
      tool_name: str, 
      payload: Dict[str, Any], 
      ctx: Dict[str, Any]
  ) -> Dict[str, Any]:
      """Central dispatcher: validate → call → wrap"""
      try:
          # (a) Validate input with Pydantic
          handler = TOOL_REGISTRY.get(tool_name)
          if not handler:
              raise ValueError(f"Unknown tool: {tool_name}")
          
          # (b) Call underlying HTTP endpoint
          result = await handler(**payload)
          
          # (c) Wrap in uniform envelope
          return format_success_envelope(result, payload)
          
      except Exception as e:
          # (d) Map exceptions to error envelope
          return format_error_envelope(str(e), payload)
  ```

### 3.2 Uniform Envelope (All Tool Responses)
- [ ] **Standardize response format**
  ```python
  def format_success_envelope(data: Any, requested_params: Dict) -> Dict:
      return {
          "meta": {
              "requested": requested_params,  # Original request params
              "applied": data.get("applied_params", requested_params),  # After caps/defaults
              "as_of": utc_now().isoformat() + "Z",
              "truncated": data.get("truncated", False),
              "limits": {
                  "symbols_max": 5,
                  "lookback_days_max": 180,
                  "timeout_ms": 3000
              },
              "retryable": False
          },
          "data": data.get("result") or data,
          "error": None
      }
  
  def format_error_envelope(message: str, requested_params: Dict, retryable: bool = False) -> Dict:
      return {
          "meta": {
              "requested": requested_params,
              "applied": {},
              "as_of": utc_now().isoformat() + "Z",
              "truncated": False,
              "limits": {"symbols_max": 5, "lookback_days_max": 180, "timeout_ms": 3000},
              "retryable": retryable
          },
          "data": None,
          "error": {
              "type": "validation_error" if "validation" in message.lower() else "execution_error",
              "message": message,
              "details": {}
          }
      }
  ```

### 3.3 Caps & Early Exit in Endpoints (Not Handlers)
- [ ] **Enhance Raw Data API endpoints with caps enforcement**
  ```python
  # In backend/app/api/v1/data.py endpoints
  
  @router.get("/prices/quotes")
  async def get_quotes(symbols: str = Query(...)):
      symbol_list = symbols.split(',')
      
      # Apply caps
      if len(symbol_list) > 5:
          applied_symbols = symbol_list[:5]
          truncated = True
          suggested_params = {"symbols": ",".join(symbol_list[:5])}
      else:
          applied_symbols = symbol_list
          truncated = False
          suggested_params = None
      
      # Set meta fields for response
      meta = {
          "requested": {"symbols": symbols},
          "applied": {"symbols": ",".join(applied_symbols)},
          "truncated": truncated,
          "suggested_params": suggested_params
      }
      
      # Process request with capped parameters
      quotes_data = await fetch_quotes(applied_symbols)
      
      return {
          "meta": meta,
          "data": quotes_data
      }
  ```

### 3.4 Per-Tool Timeouts & Retries
- [ ] **Implement httpx with timeout and retry logic**
  ```python
  import httpx
  from tenacity import retry, stop_after_attempt, wait_exponential
  
  @retry(
      stop=stop_after_attempt(3),  # Configurable per tool
      wait=wait_exponential(multiplier=1, min=1, max=4),
      retry_error_callback=lambda retry_state: {"retries": retry_state.attempt_number}
  )
  async def call_raw_data_api(endpoint: str, params: Dict, timeout: float = 3.0) -> Dict:
      """Call Raw Data API with timeout and retry"""
      async with httpx.AsyncClient(timeout=timeout) as client:
          response = await client.get(endpoint, params=params)
          
          # Set retryable=true for transient errors
          if response.status_code in [429, 500, 502, 503, 504]:
              retryable = True
              response.raise_for_status()  # Triggers retry
          elif response.status_code >= 400:
              retryable = False
              response.raise_for_status()  # No retry
          
          return response.json()
  ```

### 3.5 OpenAI Provider Adapter (Provider-Specific Layer)
- [ ] **Create `backend/app/agent/adapters/openai_adapter.py`**
  ```python
  class OpenAIToolAdapter:
      """Converts tool definitions/responses for OpenAI function calling"""
      
      def __init__(self, tools: PortfolioTools):
          self.tools = tools
          
      def get_function_schemas(self) -> List[Dict]:
          # OpenAI function calling schema format
          
      async def execute_tool(self, name: str, args: Dict) -> str:
          result = await dispatch_tool_call(name, args, {})  # Use registry
          return json.dumps(result)  # OpenAI expects JSON string
  ```

### 3.6 Tool Implementation Details (Business Logic Layer)

- [ ] **get_portfolio_complete** (ref: TDD §7.1, PRD §6.1)
  ```python
  async def get_portfolio_complete(
      portfolio_id: str,
      include_positions: bool = True,
      include_cash: bool = True,
      as_of_date: Optional[str] = None
  ) -> Dict[str, Any]:
      # Call /api/v1/data/portfolio/{portfolio_id}/complete
      # Enforce max_rows_positions=200
      # Return standardized response with meta
  ```

- [ ] **get_portfolio_data_quality**
  ```python
  async def get_portfolio_data_quality(
      portfolio_id: str,
      check_factors: bool = True,
      check_correlations: bool = True
  ) -> Dict[str, Any]:
      # Call /api/v1/data/portfolio/{portfolio_id}/data-quality
      # Return feasibility assessment
  ```

- [ ] **get_positions_details**
  ```python
  async def get_positions_details(
      portfolio_id: Optional[str] = None,
      position_ids: Optional[str] = None,
      include_closed: bool = False
  ) -> Dict[str, Any]:
      # Validate: portfolio_id OR position_ids required
      # Call /api/v1/data/positions/details
      # Enforce max_rows=200 with truncation
  ```

- [ ] **get_prices_historical**
  ```python
  async def get_prices_historical(
      portfolio_id: str,
      lookback_days: int = 150,
      include_factor_etfs: bool = True,
      date_format: str = "iso"
  ) -> Dict[str, Any]:
      # Special handling: fetch positions first
      # Identify top 5 symbols by market value
      # Call /api/v1/data/prices/historical/{portfolio_id}
      # Post-process to filter symbols
      # Set truncated=true if filtering occurred
  ```

- [ ] **get_current_quotes**
  ```python
  async def get_current_quotes(
      symbols: str,
      include_options: bool = False
  ) -> Dict[str, Any]:
      # Parse comma-separated symbols
      # Enforce max_symbols=5
      # Call /api/v1/data/prices/quotes
  ```

- [ ] **get_factor_etf_prices**
  ```python
  async def get_factor_etf_prices(
      lookback_days: int = 150,
      factors: Optional[str] = None
  ) -> Dict[str, Any]:
      # Map factor names to ETF symbols
      # Call /api/v1/data/factors/etf-prices
      # Include resolved symbols in meta.applied
  ```

### 3.7 Future Provider Support (Architecture Ready)
- [ ] **Adding New Provider (e.g., Anthropic, Gemini)** 🔮 **Future Work**
  ```python
  class AnthropicToolAdapter:
      """When needed: Anthropic XML tool format adapter"""
      def get_tool_definitions(self) -> List[str]:
          # Anthropic XML schema format
          
      async def execute_tool(self, name: str, args: Dict) -> Dict:
          result = await getattr(self.tools, name)(**args)
          return result  # Anthropic expects structured response
  
  class GeminiToolAdapter:
      """When needed: Google Gemini function format adapter"""  
      # Similar pattern with Google-specific schemas
  ```

**Migration Effort Per Provider:**
- ✅ Business logic: 0% changes (reuse existing PortfolioTools)
- 🔧 New adapter class: ~200 lines
- 🔧 Schema conversion: ~50 lines per tool  
- 🔧 Response formatting: ~20 lines per tool
- ⏱️ **Total effort: 1-2 days vs complete rewrite**

### 3.8 Tool Response Standardization (Provider-Agnostic)
- [ ] **Implement common response envelope** (used by all providers)
  ```python
  def format_tool_response(
      data: Any,
      requested_params: Dict,
      applied_params: Dict,
      limits: Dict,
      rows_returned: int,
      truncated: bool = False,
      suggested_params: Optional[Dict] = None
  ) -> Dict[str, Any]:
      return {
          "meta": {
              "as_of": utc_now().isoformat() + "Z",
              "requested": requested_params,
              "applied": applied_params,
              "limits": limits,
              "rows_returned": rows_returned,
              "truncated": truncated,
              "suggested_params": suggested_params
          },
          "data": data
      }
  ```

---

## 📋 Phase 4: Prompt Engineering (Day 8-9)

> Reference: TDD §9 (Prompt Library), PRD §5 (Prompt Modes)

### 4.1 Create Prompt Templates
- [ ] **agent/agent_pkg/prompts/**
  ```
  agent/agent_pkg/prompts/
  ├── green_v001.md       # Teaching-focused (default)
  ├── blue_v001.md        # Concise/quantitative
  ├── indigo_v001.md      # Strategic/narrative
  ├── violet_v001.md      # Risk-focused
  └── common_instructions.md
  ```

### 4.2 Green Mode
- [ ] **Create green_v001.md**
  ```yaml
  ---
  id: green
  version: v001
  mode: Green
  persona: Teaching-focused financial analyst
  token_budget: 2000
  ---
  
  # System Instructions
  - Educational, step-by-step explanations
  - Define financial terms for beginners
  - Use analogies and examples
  - Verbose but clear communication
  - Include "as of" timestamps
  ```

### 4.3 Blue Mode
- [ ] **Create blue_v001.md**
  ```yaml
  ---
  id: blue
  version: v001
  mode: Blue
  persona: Quantitative analyst
  token_budget: 1500
  ---
  
  # System Instructions
  - Concise, data-forward responses
  - Tables and numbers over prose
  - Assume professional audience
  - Technical terminology OK
  - Minimal explanations
  ```

### 4.4 Indigo Mode
- [ ] **Create indigo_v001.md**
  ```yaml
  ---
  id: indigo
  version: v001
  mode: Indigo
  persona: Strategic investment analyst
  token_budget: 1800
  ---
  
  # System Instructions
  - Focus on market context and trends
  - Narrative style with forward insights
  - Connect portfolio to macro themes
  - Scenario analysis and implications
  - Strategic recommendations
  ```

### 4.5 Violet Mode
- [ ] **Create violet_v001.md**
  ```yaml
  ---
  id: violet
  version: v001
  mode: Violet
  persona: Conservative risk analyst
  token_budget: 1700
  ---
  
  # System Instructions
  - Emphasize risks and stress scenarios
  - Conservative, cautious tone
  - Include compliance disclaimers
  - Focus on capital preservation
  - Highlight concentration risks
  ```

### 4.6 Prompt Loading System
- [ ] **Implement prompt loader**
  ```python
  class PromptManager:
      def load_prompt(mode: str) -> str
      def get_system_prompt(mode: str, user_context: Dict) -> str
      def inject_variables(prompt: str, variables: Dict) -> str
  ```

---

## 📋 Phase 5: API Documentation Sync (Ongoing)

> **IMPORTANT**: As we implement and enhance endpoints, we track progress in agent/TODO.md
> then update backend/_docs/requirements/API_SPECIFICATIONS_V1.4.4.md after completion.

### 5.1 API Endpoint Cross-Reference (Per API_SPECIFICATIONS_V1.4.4.md)

**Currently Documented Raw Data Endpoints:**
- ✅ `GET /api/v1/data/portfolio/{portfolio_id}/complete` - Matches spec
- ✅ `GET /api/v1/data/portfolio/{portfolio_id}/data-quality` - Matches spec
- ✅ `GET /api/v1/data/positions/details` - Matches spec
- ✅ `GET /api/v1/data/prices/historical/{portfolio_id}` - Needs enhancement per Agent requirements
- ✅ `GET /api/v1/data/prices/quotes` - Matches spec
- ✅ `GET /api/v1/data/factors/etf-prices` - Matches spec

**New Endpoints to Add to API Spec (After Implementation):**
- [ ] `GET /api/v1/data/positions/top/{portfolio_id}` - Top N positions by value
- [ ] `GET /api/v1/data/portfolio/{portfolio_id}/summary` - Condensed overview

**Enhancement Parameters to Document (After Implementation):**
- [ ] `/prices/historical` - Add `max_symbols`, `selection_method` parameters
- [ ] All endpoints - Add `meta` object with truncation info

### 5.2 Documentation Update Checklist
- [ ] **After each endpoint implementation:**
  - [ ] Test endpoint thoroughly
  - [ ] Update API_SPECIFICATIONS_V1.4.4.md with:
    - [ ] New parameters
    - [ ] Response schema changes
    - [ ] Meta object structure
    - [ ] Truncation behavior
  - [ ] Increment version to 1.4.5
  - [ ] Update implementation status percentage

## 📋 Phase 6: Testing & Validation (Day 9-10)

> Reference: TDD §14 (Testing), PRD §9 (Performance Targets), §13 (Golden Set)

### 6.1 Unit Tests
- [ ] **Test service layer**
  - [ ] `PortfolioDataService.get_top_positions_by_value()`
  - [ ] `PortfolioDataService.get_portfolio_summary()`
  - [ ] `PortfolioDataService.get_historical_prices_with_selection()`
  - [ ] Mock database sessions
  - [ ] Test selection methods (top_by_value, top_by_weight)

- [ ] **Test conversation management**
  - [ ] Conversation creation
  - [ ] Mode switching
  - [ ] Database persistence

- [ ] **Test tool handlers**
  - [ ] Each tool with valid params
  - [ ] Cap enforcement
  - [ ] Truncation behavior
  - [ ] Error handling

- [ ] **Test SSE streaming**
  - [ ] Connection establishment
  - [ ] Event formatting
  - [ ] Heartbeat mechanism
  - [ ] Error recovery

### 5.2 Integration Tests
- [ ] **End-to-end chat flow**
  - [ ] Login → Create conversation → Send message
  - [ ] Tool execution → Response streaming
  - [ ] Code Interpreter execution

- [ ] **Golden Test Suite (15 queries)** (ref: PRD §13 for test cases)
  1. [ ] "What's my 30-day P&L?"
  2. [ ] "Show my biggest positions"
  3. [ ] "What's my delta exposure?"
  4. [ ] "Which positions expire this week?"
  5. [ ] "Calculate my portfolio beta"
  6. [ ] "Show factor exposures"
  7. [ ] "What's my cash balance?"
  8. [ ] "Compare this week vs last week performance"
  9. [ ] "Show me AAPL historical prices"
  10. [ ] "What's my portfolio value?"
  11. [ ] "Calculate position-level returns"
  12. [ ] "Show correlation matrix"
  13. [ ] "What's my largest loss today?"
  14. [ ] "Show options positions only"
  15. [ ] "Calculate Sharpe ratio"

### 5.3 Performance Testing
- [ ] **Latency measurements** (ref: TDD §12, PRD §3 Success Metrics)
  - [ ] Stream start ≤ 3s p50
  - [ ] Complete response ≤ 8-10s p95
  - [ ] Tool execution ≤ 5-6s p95

- [ ] **Load testing**
  - [ ] Concurrent conversations
  - [ ] Rate limit validation
  - [ ] Cache effectiveness

---

## 📋 Phase 6: Telemetry & Monitoring (Day 10-11)

### 6.1 Logging Implementation
- [ ] **Structured logging**
  ```python
  logger.info("conversation_started", extra={
      "conversation_id": conv_id,
      "user_id": user_id,
      "mode": mode
  })
  ```

- [ ] **Log points**
  - [ ] Conversation creation
  - [ ] Message received
  - [ ] Tool execution start/end
  - [ ] OpenAI API calls
  - [ ] Errors and retries

### 6.2 Metrics Collection
- [ ] **Per-conversation metrics**
  - [ ] Total messages
  - [ ] Tool calls count
  - [ ] Token usage
  - [ ] Total latency
  - [ ] Error rate

- [ ] **Aggregate metrics**
  - [ ] Daily active conversations
  - [ ] Most used tools
  - [ ] Average tokens per conversation
  - [ ] Success rate by query type

### 6.3 Error Tracking
- [ ] **Error categorization**
  - [ ] OpenAI API errors
  - [ ] Tool execution failures
  - [ ] Data availability issues
  - [ ] Token limit exceeded

---

## 📋 Phase 7: Documentation (Day 11-12)

### 7.1 API Documentation
- [ ] **Document chat endpoints**
  - [ ] OpenAPI/Swagger specs
  - [ ] Request/response examples
  - [ ] Error codes

- [ ] **Tool documentation**
  - [ ] Available tools list
  - [ ] Parameter descriptions
  - [ ] Response formats
  - [ ] Limitations and caps

### 7.2 Development Guide
- [ ] **Agent development guide**
  - [ ] Architecture overview
  - [ ] Adding new tools
  - [ ] Prompt engineering tips
  - [ ] Testing strategies

### 7.3 Deployment Guide
- [ ] **Deployment instructions**
  - [ ] Environment variables
  - [ ] Database setup
  - [ ] OpenAI configuration
  - [ ] Monitoring setup

---

## 📋 Phase 8: Deployment Preparation (Day 12-14)

### 8.1 Environment Configuration
- [ ] **Production environment variables**
  - [ ] Secure OpenAI API key storage
  - [ ] Production database credentials
  - [ ] Cache configuration
  - [ ] Rate limit settings

### 8.2 Security Review
- [ ] **Security checklist**
  - [ ] API key rotation plan
  - [ ] JWT validation
  - [ ] Rate limiting
  - [ ] Input sanitization
  - [ ] PII handling

### 8.3 Performance Optimization
- [ ] **Caching strategy**
  - [ ] Tool response caching
  - [ ] Prompt template caching
  - [ ] Conversation context caching

- [ ] **Database optimization**
  - [ ] Query optimization
  - [ ] Connection pooling
  - [ ] Index verification

---

## 🚀 Success Criteria

> Reference: PRD §3 (Success Metrics), TDD §12 (Performance Limits)

### Technical Requirements
- [ ] ✅ All Raw Data APIs return real data
- [ ] ✅ Chat endpoints fully functional
- [ ] ✅ SSE streaming working smoothly
- [ ] ✅ Tool execution with proper caps
- [ ] ✅ Code Interpreter integration
- [ ] ✅ Both analyst modes working
- [ ] ✅ UTC ISO 8601 timestamps everywhere

### Performance Targets
- [ ] ✅ Stream start ≤ 3s p50
- [ ] ✅ Complete response ≤ 8-10s p95
- [ ] ✅ Tool execution ≤ 5-6s p95
- [ ] ✅ 80% pass rate on golden queries

### Quality Metrics
- [ ] ✅ No hallucinated tickers/values
- [ ] ✅ Accurate calculations via Code Interpreter
- [ ] ✅ Proper error handling with suggestions
- [ ] ✅ 70% Good/Excellent usefulness rating

---

## 📝 Notes

1. **Critical Path**: Raw Data APIs → Chat Infrastructure → Tool Handlers → Testing
2. **Blocking Issues**: Must fix GPT-5 references and complete Raw Data APIs first
3. **Dependencies**: Requires OpenAI API key and properly configured backend
4. **Risk Areas**: Historical prices symbol filtering complexity, SSE connection stability
5. **Quick Wins**: UTC standardization already complete, some APIs already return real data

---

## 🔄 Daily Standup Checklist

- [ ] Update completion percentages
- [ ] Flag any blockers
- [ ] Note any scope changes
- [ ] Update time estimates
- [ ] Document decisions made
- [ ] Plan next day's priorities

---

**Last Updated:** 2025-08-27  
**Next Review:** Daily during implementation